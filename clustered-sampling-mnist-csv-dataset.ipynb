{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34877,"sourceType":"datasetVersion","datasetId":27352}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%HTML\n<style>\nh4 {\n    --background: #F7A5A5;\n    --foreground: red;\n    background-color: var(--background);\n    border: 2px solid var(--foreground);\n    padding: 20px;\n    border-radius: 10px;\n    color: var(--foreground);\n}\nh1 {\n    --background: #20BEFFBB;\n    --foreground: blue;\n    background-color: var(--background);\n    border: 2px solid var(--foreground);\n    padding: 20px;\n    border-radius: 10px;\n    color: var(--foreground);\n}\nh3 {\n    --background: #FFCE84;\n    --foreground: #F07108;\n    background-color: var(--background);\n    border: 2px solid var(--foreground);\n    padding: 10px;\n    border-radius: 10px;\n    color: var(--foreground);\n    width: 50%;\n}\n</style>","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:32:49.928592Z","iopub.execute_input":"2023-11-29T06:32:49.929444Z","iopub.status.idle":"2023-11-29T06:32:49.976433Z","shell.execute_reply.started":"2023-11-29T06:32:49.929396Z","shell.execute_reply":"2023-11-29T06:32:49.975151Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\nh4 {\n    --background: #F7A5A5;\n    --foreground: red;\n    background-color: var(--background);\n    border: 2px solid var(--foreground);\n    padding: 20px;\n    border-radius: 10px;\n    color: var(--foreground);\n}\nh1 {\n    --background: #20BEFFBB;\n    --foreground: blue;\n    background-color: var(--background);\n    border: 2px solid var(--foreground);\n    padding: 20px;\n    border-radius: 10px;\n    color: var(--foreground);\n}\nh3 {\n    --background: #FFCE84;\n    --foreground: #F07108;\n    background-color: var(--background);\n    border: 2px solid var(--foreground);\n    padding: 10px;\n    border-radius: 10px;\n    color: var(--foreground);\n    width: 50%;\n}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Directory Structure","metadata":{}},{"cell_type":"code","source":"!mkdir saved_exp_info\n!mkdir saved_exp_info/acc\n!mkdir saved_exp_info/final_model\n!mkdir saved_exp_info/len_dbs\n!mkdir saved_exp_info/local_model_history\n!mkdir saved_exp_info/loss\n!mkdir saved_exp_info/sampled_clients\n!mkdir saved_exp_info/server_history","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:33:00.972533Z","iopub.execute_input":"2023-11-29T06:33:00.973883Z","iopub.status.idle":"2023-11-29T06:33:09.773845Z","shell.execute_reply.started":"2023-11-29T06:33:00.973833Z","shell.execute_reply":"2023-11-29T06:33:09.771716Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Clustering","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nimport numpy as np\nfrom copy import deepcopy\nfrom itertools import product\nfrom numpy.random import choice\nfrom scipy.cluster.hierarchy import fcluster\n\n\"\"\"\nFILE OUTLINE:\n1. get_clusters_with_alg1               : Algorithm 1: Client Sample Weights => Sampling Probabilities (*)\n\n-  get_similarity                       : Measures Similarity Between 2 Gradient Arrays\n\n-  get_gradients                        : Computes Gradients from Global Model and Local Model\n\n4. get_matrix_similarity_from_grads     : \n\n5. get_matrix_similarity                : \n\n6. get_clusters_with_alg2               : Algorithm 2: Client Sample Weights => Sampling Probabilities (*)\n\n7. sample_clients                       : Sample Clients from a given CLient Sampling Probability Distribution.\n\"\"\"\n\n\ndef get_clusters_with_alg1(n_sampled: int, weights: np.array):\n    \"\"\"ALGORITHM 1\"\"\"\n    epsilon = int(10**10)\n    # associate each client to a cluster\n    augmented_weights = np.array([w * n_sampled * epsilon for w in weights])\n    ordered_client_idx = np.flip(np.argsort(augmented_weights))\n    n_clients = len(weights)\n    distri_clusters = np.zeros((n_sampled, n_clients)).astype(int)\n    k = 0\n    for client_idx in ordered_client_idx:\n        while augmented_weights[client_idx] > 0:\n            sum_proba_in_k = np.sum(distri_clusters[k])\n            u_i = min(epsilon - sum_proba_in_k, augmented_weights[client_idx])\n            distri_clusters[k, client_idx] = u_i\n            augmented_weights[client_idx] += -u_i\n            sum_proba_in_k = np.sum(distri_clusters[k])\n            if sum_proba_in_k == 1 * epsilon:\n                k += 1\n    distri_clusters = distri_clusters.astype(float)\n    for l in range(n_sampled):\n        distri_clusters[l] /= np.sum(distri_clusters[l])\n    return distri_clusters\n\n\ndef get_similarity(grad_1, grad_2, distance_type=\"L1\"):\n    if distance_type == \"L1\":\n        norm = 0\n        for g_1, g_2 in zip(grad_1, grad_2):\n            norm += np.sum(np.abs(g_1 - g_2))\n        return norm\n    elif distance_type == \"L2\":\n        norm = 0\n        for g_1, g_2 in zip(grad_1, grad_2):\n            norm += np.sum((g_1 - g_2) ** 2)\n        return norm\n    elif distance_type == \"cosine\":\n        norm, norm_1, norm_2 = 0, 0, 0\n        for i in range(len(grad_1)):\n            norm += np.sum(grad_1[i] * grad_2[i])\n            norm_1 += np.sum(grad_1[i] ** 2)\n            norm_2 += np.sum(grad_2[i] ** 2)\n        if norm_1 == 0.0 or norm_2 == 0.0:\n            return 0.0\n        else:\n            norm /= np.sqrt(norm_1 * norm_2)\n            return np.arccos(norm)\n\n\ndef get_gradients(sampling, global_m, local_models):\n    \"\"\"return the `representative gradient` formed by the difference between\n    the local work and the sent global model\"\"\"\n    local_model_params = []\n    for model in local_models:\n        local_model_params += [\n            [tens.detach().numpy() for tens in list(model.parameters())]\n        ]\n    global_model_params = [\n        tens.detach().numpy() for tens in list(global_m.parameters())\n    ]\n    local_model_grads = []\n    for local_params in local_model_params:\n        local_model_grads += [\n            [\n                local_weights - global_weights\n                for local_weights, global_weights in zip(\n                    local_params, global_model_params\n                )\n            ]\n        ]\n    return local_model_grads\n\n\ndef get_matrix_similarity_from_grads(local_model_grads, distance_type):\n    \"\"\"return the similarity matrix where the distance chosen to\n    compare two clients is set with `distance_type`\"\"\"\n    n_clients = len(local_model_grads)\n    metric_matrix = np.zeros((n_clients, n_clients))\n    for i, j in product(range(n_clients), range(n_clients)):\n        metric_matrix[i, j] = get_similarity(\n            local_model_grads[i], local_model_grads[j], distance_type\n        )\n    return metric_matrix\n\n\ndef get_matrix_similarity(global_m, local_models, distance_type):\n    n_clients = len(local_models)\n    local_model_grads = get_gradients(global_m, local_models)\n    metric_matrix = np.zeros((n_clients, n_clients))\n    for i, j in product(range(n_clients), range(n_clients)):\n        metric_matrix[i, j] = get_similarity(\n            local_model_grads[i], local_model_grads[j], distance_type\n        )\n    return metric_matrix\n\n\ndef get_clusters_with_alg2(linkage_matrix: np.array, n_sampled: int, weights: np.array):\n    \"\"\"ALGORITHM 2\"\"\"\n    epsilon = int(10**10)\n    # associate each client to a cluster\n    link_matrix_p = deepcopy(linkage_matrix)\n    augmented_weights = deepcopy(weights)\n    for i in range(len(link_matrix_p)):\n        idx_1, idx_2 = int(link_matrix_p[i, 0]), int(link_matrix_p[i, 1])\n        new_weight = np.array([augmented_weights[idx_1] + augmented_weights[idx_2]])\n        augmented_weights = np.concatenate((augmented_weights, new_weight))\n        link_matrix_p[i, 2] = int(new_weight * epsilon)\n    clusters = fcluster(link_matrix_p, int(epsilon / n_sampled), criterion=\"distance\")\n    n_clients, n_clusters = len(clusters), len(set(clusters))\n    # Associate each cluster to its number of clients in the cluster\n    pop_clusters = np.zeros((n_clusters, 2)).astype(int)\n    for i in range(n_clusters):\n        pop_clusters[i, 0] = i + 1\n        for client in np.where(clusters == i + 1)[0]:\n            pop_clusters[i, 1] += int(weights[client] * epsilon * n_sampled)\n    pop_clusters = pop_clusters[pop_clusters[:, 1].argsort()]\n    distri_clusters = np.zeros((n_sampled, n_clients)).astype(int)\n    # n_sampled biggest clusters that will remain unchanged\n    kept_clusters = pop_clusters[n_clusters - n_sampled :, 0]\n    for idx, cluster in enumerate(kept_clusters):\n        for client in np.where(clusters == cluster)[0]:\n            distri_clusters[idx, client] = int(weights[client] * n_sampled * epsilon)\n    k = 0\n    for j in pop_clusters[: n_clusters - n_sampled, 0]:\n        clients_in_j = np.where(clusters == j)[0]\n        np.random.shuffle(clients_in_j)\n        for client in clients_in_j:\n            weight_client = int(weights[client] * epsilon * n_sampled)\n            while weight_client > 0:\n                sum_proba_in_k = np.sum(distri_clusters[k])\n                u_i = min(epsilon - sum_proba_in_k, weight_client)\n                distri_clusters[k, client] = u_i\n                weight_client += -u_i\n                sum_proba_in_k = np.sum(distri_clusters[k])\n                if sum_proba_in_k == 1 * epsilon:\n                    k += 1\n    distri_clusters = distri_clusters.astype(float)\n    for l in range(n_sampled):\n        distri_clusters[l] /= np.sum(distri_clusters[l])\n    return distri_clusters\n\n\ndef sample_clients(distri_clusters):\n    n_clients = len(distri_clusters[0])\n    n_sampled = len(distri_clusters)\n    sampled_clients = np.zeros(len(distri_clusters), dtype=int)\n    for k in range(n_sampled):\n        sampled_clients[k] = int(choice(n_clients, 1, p=distri_clusters[k]))\n    return sampled_clients\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:33:15.808006Z","iopub.execute_input":"2023-11-29T06:33:15.808479Z","iopub.status.idle":"2023-11-29T06:33:16.034241Z","shell.execute_reply.started":"2023-11-29T06:33:15.808441Z","shell.execute_reply":"2023-11-29T06:33:16.033221Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Fed Prox","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\nimport torch\nimport pickle\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom copy import deepcopy\nimport torch.optim as optim\n\n\ndef set_to_zero_model_weights(model):\n    \"\"\"SETTING ALL THE PARAMETERS OF THE MODEL TO 0\"\"\"\n    for layer_weigths in model.parameters():\n        layer_weigths.data.sub_(layer_weigths.data)\n\n\ndef FedAvg_agregation_process(model, clients_models_hist: list, weights: list):\n    \"\"\"Creates the new model of a given iteration with the models of the other clients\"\"\"\n    new_model = deepcopy(model)\n    set_to_zero_model_weights(new_model)\n    for k, client_hist in enumerate(clients_models_hist):\n        for idx, layer_weights in enumerate(new_model.parameters()):\n            contribution = client_hist[idx].data * weights[k]\n            layer_weights.data.add_(contribution)\n    return new_model\n\n\ndef FedAvg_agregation_process_for_FA_sampling(\n    model, clients_models_hist: list, weights: list\n):\n    \"\"\"Creates the new model of a given iteration with the models of the other\n    clients\"\"\"\n    new_model = deepcopy(model)\n    for layer_weigths in new_model.parameters():\n        layer_weigths.data.sub_(sum(weights) * layer_weigths.data)\n    for k, client_hist in enumerate(clients_models_hist):\n        for idx, layer_weights in enumerate(new_model.parameters()):\n            contribution = client_hist[idx].data * weights[k]\n            layer_weights.data.add_(contribution)\n    return new_model\n\n\ndef accuracy_dataset(model, dataset):\n    \"\"\"Compute the accuracy of `model` on `test_data`\"\"\"\n    correct = 0\n    for features, labels in dataset:\n        predictions = model(features)\n        _, predicted = predictions.max(1, keepdim=True)\n        correct += torch.sum(predicted.view(-1, 1) == labels.view(-1, 1)).item()\n    accuracy = 100 * correct / len(dataset.dataset)\n    return accuracy\n\n\ndef loss_dataset(model, train_data, loss_f):\n    \"\"\"Compute the loss of `model` on `test_data`\"\"\"\n    loss = 0\n    for idx, (features, labels) in enumerate(train_data):\n        predictions = model(features)\n        loss += loss_f(predictions, labels)\n    loss /= idx + 1\n    return loss\n\n\ndef loss_classifier(predictions, labels):\n    criterion = nn.CrossEntropyLoss()\n    return criterion(predictions, labels)\n\n\ndef n_params(model):\n    \"\"\"return the number of parameters in the model\"\"\"\n    n_params = sum(\n        [\n            np.prod([tensor.size()[k] for k in range(len(tensor.size()))])\n            for tensor in list(model.parameters())\n        ]\n    )\n    return n_params\n\n\ndef difference_models_norm_2(model_1, model_2):\n    \"\"\"Return the norm 2 difference between the two model parameters\"\"\"\n    tensor_1 = list(model_1.parameters())\n    tensor_2 = list(model_2.parameters())\n    norm = sum(\n        [torch.sum((tensor_1[i] - tensor_2[i]) ** 2) for i in range(len(tensor_1))]\n    )\n    return norm\n\n\ndef local_learning(model, mu: float, optimizer, train_data, n_SGD: int, loss_f):\n    model_0 = deepcopy(model)\n    for _ in range(n_SGD):\n        features, labels = next(iter(train_data))\n        optimizer.zero_grad()\n        predictions = model(features)\n        batch_loss = loss_f(predictions, labels)\n        batch_loss += mu / 2 * difference_models_norm_2(model, model_0)\n        batch_loss.backward()\n        optimizer.step()\n\n\ndef save_pkl(dictionnary, directory, file_name):\n    \"\"\"Save the dictionnary in the directory under the file_name with pickle\"\"\"\n    with open(f\"saved_exp_info/{directory}/{file_name}.pkl\", \"wb\") as output:\n        pickle.dump(dictionnary, output)\n\n\ndef FedProx_sampling_random(\n    model,\n    n_sampled,\n    training_sets: list,\n    testing_sets: list,\n    n_iter: int,\n    n_SGD: int,\n    lr,\n    file_name: str,\n    decay=1,\n    metric_period=1,\n    mu=0,\n):\n    \"\"\"all the clients are considered in this implementation of FedProx\n    Parameters:\n        - `model`: common structure used by the clients and the server\n        - `training_sets`: list of the training sets. At each index is the\n            trainign set of client \"index\"\n        - `n_iter`: number of iterations the server will run\n        - `testing_set`: list of the testing sets. If [], then the testing\n            accuracy is not computed\n        - `mu`: regularixation term for FedProx. mu=0 for FedAvg\n        - `epochs`: number of epochs each client is running\n        - `lr`: learning rate of the optimizer\n        - `decay`: to change the learning rate at each iteration\n\n    returns :\n        - `model`: the final global model\n    \"\"\"\n    loss_f = loss_classifier\n    K = len(training_sets)  # number of clients\n    n_samples = np.array([len(db.dataset) for db in training_sets])\n    weights = n_samples / np.sum(n_samples)\n    print(\"Clients' weights:\", weights)\n    loss_hist = np.zeros((n_iter + 1, K))\n    acc_hist = np.zeros((n_iter + 1, K))\n    for k, dl in enumerate(training_sets):\n        loss_hist[0, k] = float(loss_dataset(model, dl, loss_f).detach())\n        acc_hist[0, k] = accuracy_dataset(model, dl)\n    # LOSS AND ACCURACY OF THE INITIAL MODEL\n    server_loss = np.dot(weights, loss_hist[0])\n    server_acc = np.dot(weights, acc_hist[0])\n    print(f\"====> i: 0 Loss: {server_loss} Test Accuracy: {server_acc}\")\n    sampled_clients_hist = np.zeros((n_iter, K)).astype(int)\n    for i in range(n_iter):\n        clients_params = []\n        np.random.seed(i)\n        sampled_clients = np.random.choice(K, size=n_sampled, replace=True, p=weights)\n        for k in sampled_clients:\n            local_model = deepcopy(model)\n            local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n            local_learning(\n                local_model,\n                mu,\n                local_optimizer,\n                training_sets[k],\n                n_SGD,\n                loss_f,\n            )\n            # GET THE PARAMETER TENSORS OF THE MODEL\n            list_params = list(local_model.parameters())\n            list_params = [tens_param.detach() for tens_param in list_params]\n            clients_params.append(list_params)\n            sampled_clients_hist[i, k] = 1\n        # CREATE THE NEW GLOBAL MODEL\n        model = FedAvg_agregation_process(\n            deepcopy(model), clients_params, weights=[1 / n_sampled] * n_sampled\n        )\n        if i % metric_period == 0:\n            # COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n            for k, dl in enumerate(training_sets):\n                loss_hist[i + 1, k] = float(loss_dataset(model, dl, loss_f).detach())\n\n            for k, dl in enumerate(testing_sets):\n                acc_hist[i + 1, k] = accuracy_dataset(model, dl)\n\n            server_loss = np.dot(weights, loss_hist[i + 1])\n            server_acc = np.dot(weights, acc_hist[i + 1])\n\n            print(\n                f\"====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}\"\n            )\n\n        # DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n        lr *= decay\n    # SAVE THE DIFFERENT TRAINING HISTORY\n    #    save_pkl(models_hist, \"local_model_history\", file_name)\n    #    save_pkl(server_hist, \"server_history\", file_name)\n    save_pkl(loss_hist, \"loss\", file_name)\n    save_pkl(acc_hist, \"acc\", file_name)\n    save_pkl(sampled_clients_hist, \"sampled_clients\", file_name)\n    torch.save(model.state_dict(), f\"saved_exp_info/final_model/{file_name}.pth\")\n    return model, loss_hist, acc_hist\n\n\ndef FedProx_clustered_sampling(\n    sampling: str,\n    model,\n    n_sampled: int,\n    training_sets: list,\n    testing_sets: list,\n    n_iter: int,\n    n_SGD: int,\n    lr: float,\n    file_name: str,\n    sim_type: str,\n    iter_FP=0,\n    decay=1.0,\n    metric_period=1,\n    mu=0.0,\n):\n    \"\"\"all the clients are considered in this implementation of FedProx\n    Parameters:\n        - `model`: common structure used by the clients and the server\n        - `training_sets`: list of the training sets. At each index is the\n            trainign set of client \"index\"\n        - `n_iter`: number of iterations the server will run\n        - `testing_set`: list of the testing sets. If [], then the testing\n            accuracy is not computed\n        - `mu`: regularixation term for FedProx. mu=0 for FedAvg\n        - `epochs`: number of epochs each client is running\n        - `lr`: learning rate of the optimizer\n        - `decay`: to change the learning rate at each iteration\n\n    returns :\n        - `model`: the final global model\n    \"\"\"\n    from scipy.cluster.hierarchy import linkage\n#     from py_func.clustering import get_matrix_similarity_from_grads\n#     if sampling == \"clustered_2\":\n#         from py_func.clustering import get_clusters_with_alg2\n#     from py_func.clustering import sample_clients\n    loss_f = loss_classifier\n    # Variables initialization\n    K = len(training_sets)  # number of clients\n    n_samples = np.array([len(db.dataset) for db in training_sets])\n    weights = n_samples / np.sum(n_samples)\n    print(\"Clients' weights:\", weights)\n    loss_hist = np.zeros((n_iter + 1, K))\n    acc_hist = np.zeros((n_iter + 1, K))\n    for k, dl in enumerate(training_sets):\n        loss_hist[0, k] = float(loss_dataset(model, dl, loss_f).detach())\n        acc_hist[0, k] = accuracy_dataset(model, dl)\n    # LOSS AND ACCURACY OF THE INITIAL MODEL\n    server_loss = np.dot(weights, loss_hist[0])\n    server_acc = np.dot(weights, acc_hist[0])\n    print(f\"====> i: 0 Loss: {server_loss} Test Accuracy: {server_acc}\")\n    sampled_clients_hist = np.zeros((n_iter, K)).astype(int)\n    # INITILIZATION OF THE GRADIENT HISTORY AS A LIST OF 0\n    if sampling == \"clustered_1\":\n#         from py_func.clustering import get_clusters_with_alg1\n        distri_clusters = get_clusters_with_alg1(n_sampled, weights)\n    elif sampling == \"clustered_2\":\n#         from py_func.clustering import get_gradients\n        gradients = get_gradients(sampling, model, [model] * K)\n    for i in range(n_iter):\n        previous_global_model = deepcopy(model)\n        clients_params = []\n        clients_models = []\n        sampled_clients_for_grad = []\n        if i < iter_FP:\n            print(\"MD sampling\")\n            np.random.seed(i)\n            sampled_clients = np.random.choice(\n                K, size=n_sampled, replace=True, p=weights\n            )\n            for k in sampled_clients:\n                local_model = deepcopy(model)\n                local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n                local_learning(\n                    local_model,\n                    mu,\n                    local_optimizer,\n                    training_sets[k],\n                    n_SGD,\n                    loss_f,\n                )\n                # SAVE THE LOCAL MODEL TRAINED\n                list_params = list(local_model.parameters())\n                list_params = [tens_param.detach() for tens_param in list_params]\n                clients_params.append(list_params)\n                clients_models.append(deepcopy(local_model))\n\n                sampled_clients_for_grad.append(k)\n                sampled_clients_hist[i, k] = 1\n        else:\n            if sampling == \"clustered_2\":\n                # GET THE CLIENTS' SIMILARITY MATRIX\n                sim_matrix = get_matrix_similarity_from_grads(\n                    gradients, distance_type=sim_type\n                )\n                # GET THE DENDROGRAM TREE ASSOCIATED\n                linkage_matrix = linkage(sim_matrix, \"ward\")\n                distri_clusters = get_clusters_with_alg2(\n                    linkage_matrix, n_sampled, weights\n                )\n            for k in sample_clients(distri_clusters):\n                local_model = deepcopy(model)\n                local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n                local_learning(\n                    local_model,\n                    mu,\n                    local_optimizer,\n                    training_sets[k],\n                    n_SGD,\n                    loss_f,\n                )\n                # SAVE THE LOCAL MODEL TRAINED\n                list_params = list(local_model.parameters())\n                list_params = [tens_param.detach() for tens_param in list_params]\n                clients_params.append(list_params)\n                clients_models.append(deepcopy(local_model))\n                sampled_clients_for_grad.append(k)\n                sampled_clients_hist[i, k] = 1\n        # CREATE THE NEW GLOBAL MODEL AND SAVE IT\n        model = FedAvg_agregation_process(\n            deepcopy(model), clients_params, weights=[1 / n_sampled] * n_sampled\n        )\n        # COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n        if i % metric_period == 0:\n            for k, dl in enumerate(training_sets):\n                loss_hist[i + 1, k] = float(loss_dataset(model, dl, loss_f).detach())\n            for k, dl in enumerate(testing_sets):\n                acc_hist[i + 1, k] = accuracy_dataset(model, dl)\n            server_loss = np.dot(weights, loss_hist[i + 1])\n            server_acc = np.dot(weights, acc_hist[i + 1])\n            print(\n                f\"====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}\"\n            )\n        # UPDATE THE HISTORY OF LATEST GRADIENT\n        if sampling == \"clustered_2\":\n            gradients_i = get_gradients(sampling, previous_global_model, clients_models)\n            for idx, gradient in zip(sampled_clients_for_grad, gradients_i):\n                gradients[idx] = gradient\n\n        lr *= decay\n    # SAVE THE DIFFERENT TRAINING HISTORY\n    #    save_pkl(models_hist, \"local_model_history\", file_name)\n    #    save_pkl(server_hist, \"server_history\", file_name)\n    save_pkl(loss_hist, \"loss\", file_name)\n    save_pkl(acc_hist, \"acc\", file_name)\n    save_pkl(sampled_clients_hist, \"sampled_clients\", file_name)\n    torch.save(model.state_dict(), f\"saved_exp_info/final_model/{file_name}.pth\")\n    return model, loss_hist, acc_hist\n\n\ndef FedProx_sampling_target(\n    model,\n    n_sampled: int,\n    training_sets: list,\n    testing_sets: list,\n    n_iter: int,\n    n_SGD: int,\n    lr,\n    file_name: str,\n    decay=1,\n    mu=0,\n):\n    \"\"\"all the clients are considered in this implementation of FedProx\n    Parameters:\n        - `model`: common structure used by the clients and the server\n        - `training_sets`: list of the training sets. At each index is the\n            trainign set of client \"index\"\n        - `n_iter`: number of iterations the server will run\n        - `testing_set`: list of the testing sets. If [], then the testing\n            accuracy is not computed\n        - `mu`: regularixation term for FedProx. mu=0 for FedAvg\n        - `epochs`: number of epochs each client is running\n        - `lr`: learning rate of the optimizer\n        - `decay`: to change the learning rate at each iteration\n\n    returns :\n        - `model`: the final global model\n    \"\"\"\n    loss_f = loss_classifier\n    # Variables initialization\n    n_samples = sum([len(db.dataset) for db in training_sets])\n    weights = [len(db.dataset) / n_samples for db in training_sets]\n    print(\"Clients' weights:\", weights)\n    loss_hist = [\n        [float(loss_dataset(model, dl, loss_f).detach()) for dl in training_sets]\n    ]\n    acc_hist = [[accuracy_dataset(model, dl) for dl in testing_sets]]\n    server_hist = [\n        [tens_param.detach().numpy() for tens_param in list(model.parameters())]\n    ]\n    models_hist = []\n    sampled_clients_hist = []\n    server_loss = sum([weights[i] * loss_hist[-1][i] for i in range(len(weights))])\n    server_acc = sum([weights[i] * acc_hist[-1][i] for i in range(len(weights))])\n    print(f\"====> i: 0 Loss: {server_loss} Server Test Accuracy: {server_acc}\")\n    for i in range(n_iter):\n        clients_params = []\n        clients_models = []\n        sampled_clients_i = []\n        for j in range(n_sampled):\n            k = j * 10 + np.random.randint(10)\n            local_model = deepcopy(model)\n            local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n            local_learning(\n                local_model,\n                mu,\n                local_optimizer,\n                training_sets[k],\n                n_SGD,\n                loss_f,\n            )\n            # GET THE PARAMETER TENSORS OF THE MODEL\n            list_params = list(local_model.parameters())\n            list_params = [tens_param.detach() for tens_param in list_params]\n            clients_params.append(list_params)\n            clients_models.append(deepcopy(local_model))\n            sampled_clients_i.append(k)\n        # CREATE THE NEW GLOBAL MODEL\n        model = FedAvg_agregation_process(\n            deepcopy(model), clients_params, weights=[1 / n_sampled] * n_sampled\n        )\n        models_hist.append(clients_models)\n        # COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n        loss_hist += [\n            [float(loss_dataset(model, dl, loss_f).detach()) for dl in training_sets]\n        ]\n        acc_hist += [[accuracy_dataset(model, dl) for dl in testing_sets]]\n        server_loss = sum([weights[i] * loss_hist[-1][i] for i in range(len(weights))])\n        server_acc = sum([weights[i] * acc_hist[-1][i] for i in range(len(weights))])\n        print(f\"====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}\")\n        server_hist.append(deepcopy(model))\n        sampled_clients_hist.append(sampled_clients_i)\n        # DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n        lr *= decay\n    # SAVE THE DIFFERENT TRAINING HISTORY\n    #    save_pkl(models_hist, \"local_model_history\", file_name)\n    #    save_pkl(server_hist, \"server_history\", file_name)\n    save_pkl(loss_hist, \"loss\", file_name)\n    save_pkl(acc_hist, \"acc\", file_name)\n    save_pkl(sampled_clients_hist, \"sampled_clients\", file_name)\n    torch.save(model.state_dict(), f\"saved_exp_info/final_model/{file_name}.pth\")\n    return model, loss_hist, acc_hist\n\n\ndef FedProx_FedAvg_sampling(\n    model,\n    n_sampled,\n    training_sets: list,\n    testing_sets: list,\n    n_iter: int,\n    n_SGD: int,\n    lr,\n    file_name: str,\n    decay=1,\n    metric_period=1,\n    mu=0,\n):\n    \"\"\"all the clients are considered in this implementation of FedProx\n    Parameters:\n        - `model`: common structure used by the clients and the server\n        - `training_sets`: list of the training sets. At each index is the\n            trainign set of client \"index\"\n        - `n_iter`: number of iterations the server will run\n        - `testing_set`: list of the testing sets. If [], then the testing\n            accuracy is not computed\n        - `mu`: regularixation term for FedProx. mu=0 for FedAvg\n        - `epochs`: number of epochs each client is running\n        - `lr`: learning rate of the optimizer\n        - `decay`: to change the learning rate at each iteration\n\n    returns :\n        - `model`: the final global model\n    \"\"\"\n    loss_f = loss_classifier\n    K = len(training_sets)  # number of clients\n    n_samples = np.array([len(db.dataset) for db in training_sets])\n    weights = n_samples / np.sum(n_samples)\n    print(\"Clients' weights:\", weights)\n    loss_hist = np.zeros((n_iter + 1, K))\n    acc_hist = np.zeros((n_iter + 1, K))\n    for k, dl in enumerate(training_sets):\n        loss_hist[0, k] = float(loss_dataset(model, dl, loss_f).detach())\n        acc_hist[0, k] = accuracy_dataset(model, dl)\n    # LOSS AND ACCURACY OF THE INITIAL MODEL\n    server_loss = np.dot(weights, loss_hist[0])\n    server_acc = np.dot(weights, acc_hist[0])\n    print(f\"====> i: 0 Loss: {server_loss} Test Accuracy: {server_acc}\")\n    sampled_clients_hist = np.zeros((n_iter, K)).astype(int)\n    for i in range(n_iter):\n        clients_params = []\n        np.random.seed(i)\n        sampled_clients = random.sample([x for x in range(K)], n_sampled)\n        print(\"sampled clients\", sampled_clients)\n        for k in sampled_clients:\n            local_model = deepcopy(model)\n            local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n            local_learning(\n                local_model,\n                mu,\n                local_optimizer,\n                training_sets[k],\n                n_SGD,\n                loss_f,\n            )\n            # GET THE PARAMETER TENSORS OF THE MODEL\n            list_params = list(local_model.parameters())\n            list_params = [tens_param.detach() for tens_param in list_params]\n            clients_params.append(list_params)\n\n            sampled_clients_hist[i, k] = 1\n        # CREATE THE NEW GLOBAL MODEL\n        model = FedAvg_agregation_process_for_FA_sampling(\n            deepcopy(model),\n            clients_params,\n            weights=[weights[client] for client in sampled_clients],\n        )\n        if i % metric_period == 0:\n            # COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n            for k, dl in enumerate(training_sets):\n                loss_hist[i + 1, k] = float(loss_dataset(model, dl, loss_f).detach())\n\n            for k, dl in enumerate(testing_sets):\n                acc_hist[i + 1, k] = accuracy_dataset(model, dl)\n\n            server_loss = np.dot(weights, loss_hist[i + 1])\n            server_acc = np.dot(weights, acc_hist[i + 1])\n\n            print(\n                f\"====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}\"\n            )\n\n        # DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n        lr *= decay\n\n    # SAVE THE DIFFERENT TRAINING HISTORY\n    #    save_pkl(models_hist, \"local_model_history\", file_name)\n    #    save_pkl(server_hist, \"server_history\", file_name)\n    save_pkl(loss_hist, \"loss\", file_name)\n    save_pkl(acc_hist, \"acc\", file_name)\n    save_pkl(sampled_clients_hist, \"sampled_clients\", file_name)\n    torch.save(model.state_dict(), f\"saved_exp_info/final_model/{file_name}.pth\")\n    return model, loss_hist, acc_hist\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:33:16.977752Z","iopub.execute_input":"2023-11-29T06:33:16.978201Z","iopub.status.idle":"2023-11-29T06:33:20.626005Z","shell.execute_reply.started":"2023-11-29T06:33:16.978160Z","shell.execute_reply":"2023-11-29T06:33:20.624623Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass NN(nn.Module):\n    def __init__(self, layer_1, layer_2):\n        super(NN, self).__init__()\n        self.fc1 = nn.Linear(784, layer_1)\n        self.fc3 = nn.Linear(layer_1, 10)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x.view(-1, 784)))\n        x = self.fc3(x)\n        return x\n\n\n# class CNN_CIFAR(torch.nn.Module):\n#   \"\"\"Model Used by the paper introducing FedAvg\"\"\"\n#   def __init__(self):\n#        super(CNN_CIFAR, self).__init__()\n#        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32, kernel_size=(3,3))\n#        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3))\n#        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3))\n#\n#        self.fc1 = nn.Linear(4*4*64, 64)\n#        self.fc2 = nn.Linear(64, 10)\n#\n#   def forward(self, x):\n#        x = F.relu(self.conv1(x))\n#        x = F.max_pool2d(x, 2, 2)\n#\n#        x = F.relu(self.conv2(x))\n#        x = F.max_pool2d(x, 2, 2)\n#\n#        x=self.conv3(x)\n#        x = x.view(-1, 4*4*64)\n#\n#        x = F.relu(self.fc1(x))\n#\n#        x = self.fc2(x)\n#        return x\n\n\nclass CNN_CIFAR_dropout(torch.nn.Module):\n    \"\"\"Model Used by the paper introducing FedAvg\"\"\"\n\n    def __init__(self):\n        super(CNN_CIFAR_dropout, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels=3, out_channels=32, kernel_size=(3, 3)\n        )\n        self.conv2 = nn.Conv2d(\n            in_channels=32, out_channels=64, kernel_size=(3, 3)\n        )\n        self.conv3 = nn.Conv2d(\n            in_channels=64, out_channels=64, kernel_size=(3, 3)\n        )\n\n        self.fc1 = nn.Linear(4 * 4 * 64, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = self.dropout(x)\n\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = self.dropout(x)\n\n        x = self.conv3(x)\n        x = self.dropout(x)\n        x = x.view(-1, 4 * 4 * 64)\n\n        x = F.relu(self.fc1(x))\n\n        x = self.fc2(x)\n        return x\n\n\ndef load_model(dataset, seed):\n    torch.manual_seed(seed)\n    if dataset == \"MNIST_shard\" or dataset == \"MNIST_iid\":\n        model = NN(50, 10)\n    elif dataset[:7] == \"CIFAR10\":\n        #        model = CNN_CIFAR()\n        model = CNN_CIFAR_dropout()\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:33:20.630533Z","iopub.execute_input":"2023-11-29T06:33:20.631245Z","iopub.status.idle":"2023-11-29T06:33:20.650342Z","shell.execute_reply.started":"2023-11-29T06:33:20.631206Z","shell.execute_reply":"2023-11-29T06:33:20.648871Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport torch\nimport pickle\nimport numpy as np\nfrom copy import deepcopy\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n#\n# -------------------------- MNIST NON-IID --------------------------\n#\n\n\ndef get_1shard(ds, row_0: int, digit: int, samples: int):\n    \"\"\"__UTILITY FUNCTION__: return an array from `ds` of `digit` starting of `row_0` in the indices of `ds`\"\"\"\n    # STEP 1\n    row = row_0  # STARTING ROW INDEX\n    shard = list()  # ACTUAL SAMPLES FROM MAIN DATASET\n    # STEP 2\n    while len(shard) < samples:  # COLLECT SAMPLES OF SAME DIGIT FROM 'ds'\n        if ds.train_labels[row] == digit:\n            shard.append(ds.train_data[row].numpy())\n        row += 1\n    # STEP 3\n    return row, shard  # RETURN THE (END INDEX, THE DATA CHUNK OF SIZE 'samples')\n\n\ndef create_MNIST_ds_1shard_per_client(n_clients, samples_train, samples_test):\n    # STEP 1: LOAD TRAIN AND TEST DATASET OF 'MNIST'\n    MNIST_train = datasets.MNIST(root=\"./data\", train=True, download=True)\n    MNIST_test = datasets.MNIST(root=\"./data\", train=False, download=True)\n    # STEP 2\n    shards_train, shards_test = [], []  # CLIENTS FROM 1 TO 100\n    labels = []  # LABELS OF EACH CLIENT (TOTAL CLIENTS = 100)\n    # STEP 3\n    for i in range(10):  # FOR EACH DIGIT CREATE 10 CLIENTS\n        row_train, row_test = 0, 0\n        for j in range(10):\n            row_train, shard_train = get_1shard(\n                MNIST_train, row_train, i, samples_train\n            )  # COLLECT A TRAIN SHARD FOR EACH CLIENT\n            row_test, shard_test = get_1shard(\n                MNIST_test, row_test, i, samples_test\n            )  # COLLECT A TEST SHARD FOR EACH CLIENT\n            # STORE THE COLLECTED SHARDS\n            shards_train.append([shard_train])\n            shards_test.append([shard_test])\n            # STORE THE LABEL\n            labels += [[i]]\n    # STEP 4: DATA VARIABLES\n    X_train = np.array(shards_train)\n    X_test = np.array(shards_test)\n    y_train = deepcopy(labels)\n    y_test = deepcopy(labels)\n    # STEP 5: SAVE THE VARIABLES TO FLIE\n    out_folder = \"./data/\"\n    train_path = f\"MNIST_shard_train_{n_clients}_{samples_train}.pkl\"\n    with open(out_folder + train_path, \"wb\") as output:\n        pickle.dump((X_train, y_train), output)\n    test_path = f\"MNIST_shard_test_{n_clients}_{samples_test}.pkl\"\n    with open(out_folder + test_path, \"wb\") as output:\n        pickle.dump((X_test, y_test), output)\n\n\ndef create_MNIST_small_niid(\n    n_clients: int,\n    samples_train: list,\n    samples_test: list,\n    clients_digits: list,\n):\n    MNIST_train = datasets.MNIST(root=\"./data\", train=True, download=True)\n    MNIST_test = datasets.MNIST(root=\"./data\", train=False, download=True)\n    X_train, X_test = [], []\n    y_train, y_test = [], []\n    for digits, n_train, n_test in zip(clients_digits, samples_train, samples_test):\n        client_samples_train, client_samples_test = [], []\n        client_labels_train, client_labels_test = [], []\n        n_train_per_shard = int(n_train / len(digits))\n        n_test_per_shard = int(n_test / len(digits))\n        for digit in digits:\n            row_train, row_test = 0, 0\n            _, shard_train = get_1shard(\n                MNIST_train, row_train, digit, n_train_per_shard\n            )\n            _, shard_test = get_1shard(MNIST_test, row_test, digit, n_test_per_shard)\n            client_samples_train += shard_train\n            client_samples_test += shard_test\n            client_labels_train += [digit] * n_train_per_shard\n            client_labels_test += [digit] * n_test_per_shard\n        X_train.append(client_samples_train)\n        X_test.append(client_samples_test)\n        y_train.append(client_labels_train)\n        y_test.append(client_labels_test)\n    folder = \"./data/\"\n    train_path = f\"MNIST_small_shard_train_{n_clients}_{samples_train}.pkl\"\n    with open(folder + train_path, \"wb\") as output:\n        pickle.dump((np.array(X_train), np.array(y_train)), output)\n    test_path = f\"MNIST_small_shard_test_{n_clients}_{samples_test}.pkl\"\n    with open(folder + test_path, \"wb\") as output:\n        pickle.dump((np.array(X_test), np.array(y_test)), output)\n\n\nclass MnistShardDataset(Dataset):\n    \"\"\"Convert the MNIST pkl file into a Pytorch Dataset\"\"\"\n    def __init__(self, file_path, k):\n        with open(file_path, \"rb\") as pickle_file:\n            dataset = pickle.load(pickle_file)\n            self.features = np.vstack(dataset[0][k])\n            vector_labels = list()\n            for idx, digit in enumerate(dataset[1][k]):\n                vector_labels += [digit] * len(dataset[0][k][idx])\n            self.labels = np.array(vector_labels)\n    def __len__(self):\n        return len(self.features)\n    def __getitem__(self, idx):\n        # 3D input 1x28x28\n        x = torch.Tensor([self.features[idx]]) / 255\n        y = torch.LongTensor([self.labels[idx]])[0]\n\n        return x, y\n\n\ndef clients_set_MNIST_shard(file_name, n_clients, batch_size=100, shuffle=True):\n    \"\"\"Download for all the clients their respective dataset\"\"\"\n    print(file_name)\n    list_dl = list()\n    for k in range(n_clients):\n        dataset_object = MnistShardDataset(file_name, k)\n        dataset_dl = DataLoader(dataset_object, batch_size=batch_size, shuffle=shuffle)\n        list_dl.append(dataset_dl)\n    return list_dl\n\n\n#\n# ----------------------------------------- CIFAR 10 Dirichilet distribution -------------------------------------------\n#\n\n\ndef partition_CIFAR_dataset(\n    dataset,\n    file_name: str,\n    balanced: bool,\n    matrix,\n    n_clients: int,\n    n_classes: int,\n    train: bool,\n):\n    \"\"\"Partition dataset into `n_clients`.\n    Each client i has matrix[k, i] of data of class k\"\"\"\n    list_clients_X = [[] for i in range(n_clients)]\n    list_clients_y = [[] for i in range(n_clients)]\n    if balanced:\n        n_samples = [500] * n_clients\n    elif not balanced and train:\n        n_samples = [100] * 10 + [250] * 30 + [500] * 30 + [750] * 20 + [1000] * 10\n    elif not balanced and not train:\n        n_samples = [20] * 10 + [50] * 30 + [100] * 30 + [150] * 20 + [200] * 10\n    list_idx = []\n    for k in range(n_classes):\n        idx_k = np.where(np.array(dataset.targets) == k)[0]\n        list_idx += [idx_k]\n    for idx_client, n_sample in enumerate(n_samples):\n        clients_idx_i = []\n        client_samples = 0\n        for k in range(n_classes):\n            if k < 9:\n                samples_digit = int(matrix[idx_client, k] * n_sample)\n            if k == 9:\n                samples_digit = n_sample - client_samples\n            client_samples += samples_digit\n            clients_idx_i = np.concatenate(\n                (clients_idx_i, np.random.choice(list_idx[k], samples_digit))\n            )\n        clients_idx_i = clients_idx_i.astype(int)\n        for idx_sample in clients_idx_i:\n            list_clients_X[idx_client] += [dataset.data[idx_sample]]\n            list_clients_y[idx_client] += [dataset.targets[idx_sample]]\n        list_clients_X[idx_client] = np.array(list_clients_X[idx_client])\n    folder = \"./data/\"\n    with open(folder + file_name, \"wb\") as output:\n        pickle.dump((list_clients_X, list_clients_y), output)\n\ndef create_CIFAR10_dirichlet(\n    dataset_name: str,\n    balanced: bool,\n    alpha: float,\n    n_clients: int,\n    n_classes: int,\n):\n    \"\"\"Create a CIFAR dataset partitioned according to a\n    dirichilet distribution Dir(alpha)\"\"\"\n    from numpy.random import dirichlet\n    matrix = dirichlet([alpha] * n_classes, size=n_clients)\n    CIFAR10_train = datasets.CIFAR10(\n        root=\"./data\",\n        train=True,\n        download=True,\n        transform=transforms.ToTensor(),\n    )\n    CIFAR10_test = datasets.CIFAR10(\n        root=\"./data\",\n        train=False,\n        download=True,\n        transform=transforms.ToTensor(),\n    )\n    file_name_train = f\"{dataset_name}_train_{n_clients}.pkl\"\n    partition_CIFAR_dataset(\n        CIFAR10_train,\n        file_name_train,\n        balanced,\n        matrix,\n        n_clients,\n        n_classes,\n        True,\n    )\n    file_name_test = f\"{dataset_name}_test_{n_clients}.pkl\"\n    partition_CIFAR_dataset(\n        CIFAR10_test,\n        file_name_test,\n        balanced,\n        matrix,\n        n_clients,\n        n_classes,\n        False,\n    )\n\n\nclass CIFARDataset(Dataset):\n    \"\"\"Convert the CIFAR pkl file into a Pytorch Dataset\"\"\"\n    def __init__(self, file_path: str, k: int):\n        dataset = pickle.load(open(file_path, \"rb\"))\n        self.X = dataset[0][k]\n        self.y = np.array(dataset[1][k])\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx: int):\n        # 3D input 32x32x3\n        x = torch.Tensor(self.X[idx]).permute(2, 0, 1) / 255\n        x = (x - 0.5) / 0.5\n        y = self.y[idx]\n        return x, y\n\n\ndef clients_set_CIFAR(file_name: str, n_clients: int, batch_size: int, shuffle=True):\n    \"\"\"Download for all the clients their respective dataset\"\"\"\n    print(file_name)\n    list_dl = list()\n    for k in range(n_clients):\n        dataset_object = CIFARDataset(file_name, k)\n        dataset_dl = DataLoader(dataset_object, batch_size=batch_size, shuffle=shuffle)\n        list_dl.append(dataset_dl)\n    return list_dl\n\n\n#\n# --------------------------------------- Upload any dataset Puts all the function above together ------------------------------------------\n#\n\n\ndef get_dataloaders(dataset, batch_size: int, shuffle=True):\n    folder = \"./data/\"\n    if dataset == \"MNIST_iid\":\n        n_clients = 100\n        samples_train, samples_test = 600, 100\n        mnist_trainset = datasets.MNIST(\n            root=\"./data\",\n            train=True,\n            download=True,\n            transform=transforms.ToTensor(),\n        )\n        mnist_train_split = torch.utils.data.random_split(\n            mnist_trainset, [samples_train] * n_clients\n        )\n        list_dls_train = [\n            torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n            for ds in mnist_train_split\n        ]\n        mnist_testset = datasets.MNIST(\n            root=\"./data\",\n            train=False,\n            download=True,\n            transform=transforms.ToTensor(),\n        )\n        mnist_test_split = torch.utils.data.random_split(\n            mnist_testset, [samples_test] * n_clients\n        )\n        list_dls_test = [\n            torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n            for ds in mnist_test_split\n        ]\n    elif dataset == \"MNIST_shard\":\n        n_clients = 100\n        samples_train, samples_test = 500, 80\n        file_name_train = f\"MNIST_shard_train_{n_clients}_{samples_train}.pkl\"\n        path_train = folder + file_name_train\n        file_name_test = f\"MNIST_shard_test_{n_clients}_{samples_test}.pkl\"\n        path_test = folder + file_name_test\n        if not os.path.isfile(path_train):\n            create_MNIST_ds_1shard_per_client(n_clients, samples_train, samples_test)\n        list_dls_train = clients_set_MNIST_shard(\n            path_train, n_clients, batch_size=batch_size, shuffle=shuffle\n        )\n        list_dls_test = clients_set_MNIST_shard(\n            path_test, n_clients, batch_size=batch_size, shuffle=shuffle\n        )\n    elif dataset == \"CIFAR10_iid\":\n        n_clients = 100\n        samples_train, samples_test = 500, 100\n        CIFAR10_train = datasets.CIFAR10(\n            root=\"./data\",\n            train=True,\n            download=True,\n            transform=transforms.ToTensor(),\n        )\n        CIFAR10_train_split = torch.utils.data.random_split(\n            CIFAR10_train, [samples_train] * n_clients\n        )\n        list_dls_train = [\n            torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n            for ds in CIFAR10_train_split\n        ]\n        CIFAR10_test = datasets.CIFAR10(\n            root=\"./data\",\n            train=False,\n            download=True,\n            transform=transforms.ToTensor(),\n        )\n        CIFAR10_test_split = torch.utils.data.random_split(\n            CIFAR10_test, [samples_test] * n_clients\n        )\n        list_dls_test = [\n            torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n            for ds in CIFAR10_test_split\n        ]\n    elif dataset[:5] == \"CIFAR\":\n        n_classes = 10\n        n_clients = 100\n        balanced = dataset[8:12] == \"bbal\"\n        alpha = float(dataset[13:])\n        file_name_train = f\"{dataset}_train_{n_clients}.pkl\"\n        path_train = folder + file_name_train\n        file_name_test = f\"{dataset}_test_{n_clients}.pkl\"\n        path_test = folder + file_name_test\n        if not os.path.isfile(path_train):\n            print(\"creating dataset alpha:\", alpha)\n            create_CIFAR10_dirichlet(dataset, balanced, alpha, n_clients, n_classes)\n        list_dls_train = clients_set_CIFAR(path_train, n_clients, batch_size, True)\n        list_dls_test = clients_set_CIFAR(path_test, n_clients, batch_size, True)\n    \n    # Save in a file the number of samples owned per client\n    list_len = list()\n    for dl in list_dls_train:\n        list_len.append(len(dl.dataset))\n    with open(f\"./saved_exp_info/len_dbs/{dataset}.pkl\", \"wb\") as output:\n        pickle.dump(list_len, output)\n    return list_dls_train, list_dls_test","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:33:20.652463Z","iopub.execute_input":"2023-11-29T06:33:20.652914Z","iopub.status.idle":"2023-11-29T06:33:21.168874Z","shell.execute_reply.started":"2023-11-29T06:33:20.652879Z","shell.execute_reply":"2023-11-29T06:33:21.167442Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\ndef get_hyperparams(dataset, n_SGD):\n    \"\"\"return the different hyperparameters considered for the experiments.\n    This function enables the user to put less input to FL_CS2.py\"\"\"\n    batch_size = 50\n    if dataset == \"MNIST_iid\":\n        n_iter = 600\n    elif dataset == \"MNIST_shard\":\n        n_iter = 600\n    elif dataset[:5] == \"CIFAR\":\n        n_iter = 1000\n    if dataset[:5] == \"CIFAR\":\n        if n_SGD <= 10:\n            n_iter = int(n_iter * 2)\n        elif n_SGD >= 200:\n            n_iter = int(n_iter / 2)\n        if n_SGD >= 200:\n            metric_period = 3\n        elif n_SGD == 100:\n            metric_period = 5\n        elif n_SGD == 50:\n            metric_period = 5\n        elif n_SGD <= 10:\n            metric_period = 10\n    else:\n        if n_SGD <= 10:\n            n_iter = int(n_iter * 2)\n        elif n_SGD >= 100:\n            n_iter = int(n_iter / 2)\n        metric_period = 2\n    n_iter = 100\n    return n_iter, batch_size, metric_period\n\n\ndef get_file_name(\n    dataset: str,\n    sampling: str,\n    sim_type: str,\n    seed: int,\n    n_SGD: int,\n    lr: float,\n    decay: float,\n    p: float,\n    mu: float,\n):\n    \"\"\"return the file name under which the experiment with these info is saved\n    under\"\"\"\n    n_iter, batch_size, meas_perf_period = get_hyperparams(dataset, n_SGD)\n    file_name = (\n        f\"{dataset}_{sampling}_{sim_type}_i{n_iter}_N{n_SGD}_lr{lr}\"\n        + f\"_B{batch_size}_d{decay}_p{p}_m{meas_perf_period}_{seed}\"\n    )\n    if mu != 0.0:\n        file_name += f\"_{mu}\"\n    return file_name\n\n\ndef get_CIFAR10_alphas():\n    \"\"\"Return the different alpha considered for the dirichlet distribution\"\"\"\n    return [0.001, 0.01, 0.1, 10.0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:33:21.171408Z","iopub.execute_input":"2023-11-29T06:33:21.171865Z","iopub.status.idle":"2023-11-29T06:33:21.186862Z","shell.execute_reply.started":"2023-11-29T06:33:21.171829Z","shell.execute_reply":"2023-11-29T06:33:21.185278Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Expermentation (Clustered 1)","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\nprint(\"dataset - sampling - sim_type - seed - n_SGD - lr - decay - p - force - mu\")\n# HYPER PARAMETERS\ndataset = \"MNIST_shard\"\nsampling = \"clustered_1\"\nsim_type = \"cosine\"\nseed = int(0)\nn_SGD = int(50)\nlr = float(0.01)\ndecay = float(0.95)\np = float(0.1)\nforce = False\n\ntry:\n    mu = float(sys.argv[10])\nexcept:\n    mu = 0.0\n\"\"\"GET THE HYPERPARAMETERS\"\"\"\nn_iter, batch_size, meas_perf_period = get_hyperparams(dataset, n_SGD)\nn_iter = 150\nprint(\"number of iterations\", n_iter)\nprint(\"batch size\", batch_size)\nprint(\"percentage of sampled clients\", p)\nprint(\"metric_period\", meas_perf_period)\nprint(\"regularization term\", mu)\n\n\n\"\"\"NAME UNDER WHICH THE EXPERIMENT'S VARIABLES WILL BE SAVED\"\"\"\nfile_name = get_file_name(dataset, sampling, sim_type, seed, n_SGD, lr, decay, p, mu)\nprint(file_name)\n\n\n\"\"\"GET THE DATASETS USED FOR THE FL TRAINING\"\"\"\nlist_dls_train, list_dls_test = get_dataloaders(dataset, batch_size)\n\n\n\"\"\"NUMBER OF SAMPLED CLIENTS\"\"\"\nn_sampled = int(p * len(list_dls_train))\nprint(\"number fo sampled clients\", n_sampled)\n\n\n\"\"\"LOAD THE INTIAL GLOBAL MODEL\"\"\"\nmodel_0 = load_model(dataset, seed)\nprint(model_0)\n\n\n\"\"\"1. FEDAVG with random sampling\"\"\"\nif sampling == \"random\" and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_sampling_random(\n        model_0, n_sampled, list_dls_train, list_dls_test, n_iter,\n        n_SGD, lr, file_name, decay, meas_perf_period, mu,\n    )\n\n\n\"\"\"2. FEDAVG with clustered sampling\"\"\"\nif (sampling == \"clustered_1\" or sampling == \"clustered_2\") and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_clustered_sampling(\n        sampling, model_0, n_sampled, list_dls_train, list_dls_test, \n        n_iter, n_SGD, lr, file_name, sim_type, 0, decay, meas_perf_period, mu,\n    )\n\n\n\"\"\"3. FEDAVG with perfect sampling for MNIST-shard\"\"\"\nif (\n    sampling == \"perfect\"\n    and dataset == \"MNIST_shard\"\n    and (not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force)\n):\n    FedProx_sampling_target(\n        model_0, n_sampled, list_dls_train,\n        list_dls_test, n_iter, n_SGD, lr, file_name, decay, mu,\n    )\n\n\n\"\"\"4. FEDAVG with its original sampling scheme sampling clients uniformly\"\"\"\nif sampling == \"FedAvg\" and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_FedAvg_sampling( \n        model_0, n_sampled, list_dls_train, list_dls_test, n_iter,\n        n_SGD, lr, file_name, decay, meas_perf_period, mu,\n    )\n\nprint(\"EXPERIMENT IS FINISHED\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:36:25.946331Z","iopub.execute_input":"2023-11-29T06:36:25.947098Z","iopub.status.idle":"2023-11-29T07:29:52.673047Z","shell.execute_reply.started":"2023-11-29T06:36:25.947052Z","shell.execute_reply":"2023-11-29T07:29:52.671809Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"dataset - sampling - sim_type - seed - n_SGD - lr - decay - p - force - mu\nnumber of iterations 150\nbatch size 50\npercentage of sampled clients 0.1\nmetric_period 2\nregularization term 0.0\nMNIST_shard_clustered_1_cosine_i100_N50_lr0.01_B50_d0.95_p0.1_m2_0\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 116321400.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 63957599.70it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 39055621.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 12898123.74it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n/opt/conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n  warnings.warn(\"train_labels has been renamed targets\")\n/opt/conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n","output_type":"stream"},{"name":"stdout","text":"./data/MNIST_shard_train_100_500.pkl\n./data/MNIST_shard_test_100_80.pkl\nnumber fo sampled clients 10\nNN(\n  (fc1): Linear(in_features=784, out_features=50, bias=True)\n  (fc3): Linear(in_features=50, out_features=10, bias=True)\n)\nClients' weights: [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n 0.01 0.01]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1707337770.py:121: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n  x = torch.Tensor([self.features[idx]]) / 255\n","output_type":"stream"},{"name":"stdout","text":"====> i: 0 Loss: 2.3104257059097293 Test Accuracy: 11.512\n====> i: 1 Loss: 2.3250331842899326 Server Test Accuracy: 14.3\n====> i: 3 Loss: 2.30852705001831 Server Test Accuracy: 20.925\n====> i: 5 Loss: 2.1838725531101226 Server Test Accuracy: 16.2\n====> i: 7 Loss: 2.0599482285976416 Server Test Accuracy: 33.875\n====> i: 9 Loss: 2.0277855277061465 Server Test Accuracy: 28.6125\n====> i: 11 Loss: 1.9840928876399992 Server Test Accuracy: 32.3875\n====> i: 13 Loss: 1.978047275543213 Server Test Accuracy: 37.03750000000001\n====> i: 15 Loss: 1.8129161882400513 Server Test Accuracy: 42.550000000000004\n====> i: 17 Loss: 1.9264506918191913 Server Test Accuracy: 35.6875\n====> i: 19 Loss: 1.8284002989530561 Server Test Accuracy: 31.875\n====> i: 21 Loss: 1.8244608396291733 Server Test Accuracy: 26.862499999999997\n====> i: 23 Loss: 1.715318659543991 Server Test Accuracy: 41.762499999999996\n====> i: 25 Loss: 1.6783855563402177 Server Test Accuracy: 47.3875\n====> i: 27 Loss: 1.6669630360603334 Server Test Accuracy: 49.075\n====> i: 29 Loss: 1.6346027976274489 Server Test Accuracy: 57.3875\n====> i: 31 Loss: 1.5633754205703734 Server Test Accuracy: 61.53750000000001\n====> i: 33 Loss: 1.5404164803028109 Server Test Accuracy: 62.300000000000004\n====> i: 35 Loss: 1.5258205395936963 Server Test Accuracy: 60.137499999999996\n====> i: 37 Loss: 1.539493094086647 Server Test Accuracy: 58.3125\n====> i: 39 Loss: 1.4819892221689226 Server Test Accuracy: 62.075\n====> i: 41 Loss: 1.4723720955848694 Server Test Accuracy: 65.9375\n====> i: 43 Loss: 1.4485146290063857 Server Test Accuracy: 66.42500000000001\n====> i: 45 Loss: 1.5106061917543412 Server Test Accuracy: 50.612500000000004\n====> i: 47 Loss: 1.4852527433633804 Server Test Accuracy: 53.65\n====> i: 49 Loss: 1.4022884720563888 Server Test Accuracy: 70.675\n====> i: 51 Loss: 1.424796736240387 Server Test Accuracy: 56.7875\n====> i: 53 Loss: 1.414557550251484 Server Test Accuracy: 60.05\n====> i: 55 Loss: 1.4470056813955308 Server Test Accuracy: 58.412499999999994\n====> i: 57 Loss: 1.3964096343517307 Server Test Accuracy: 60.175000000000004\n====> i: 59 Loss: 1.4016686177253723 Server Test Accuracy: 59.85000000000001\n====> i: 61 Loss: 1.3299631035327912 Server Test Accuracy: 69.375\n====> i: 63 Loss: 1.3380376291275022 Server Test Accuracy: 68.4125\n====> i: 65 Loss: 1.3111694139242174 Server Test Accuracy: 73.8625\n====> i: 67 Loss: 1.3066254782676698 Server Test Accuracy: 72.86250000000001\n====> i: 69 Loss: 1.2912293148040772 Server Test Accuracy: 72.42500000000001\n====> i: 71 Loss: 1.291358650922775 Server Test Accuracy: 74.925\n====> i: 73 Loss: 1.2843114650249483 Server Test Accuracy: 73.65\n====> i: 75 Loss: 1.2684034645557403 Server Test Accuracy: 76.38749999999999\n====> i: 77 Loss: 1.2658571189641954 Server Test Accuracy: 76.3375\n====> i: 79 Loss: 1.268562161922455 Server Test Accuracy: 70.25000000000001\n====> i: 81 Loss: 1.2652376115322117 Server Test Accuracy: 70.7625\n====> i: 83 Loss: 1.2639460051059723 Server Test Accuracy: 72.52499999999999\n====> i: 85 Loss: 1.2466993451118469 Server Test Accuracy: 80.07499999999999\n====> i: 87 Loss: 1.247133951187134 Server Test Accuracy: 79.86250000000001\n====> i: 89 Loss: 1.2548157858848572 Server Test Accuracy: 76.38750000000002\n====> i: 91 Loss: 1.2551991057395933 Server Test Accuracy: 72.6875\n====> i: 93 Loss: 1.2474009883403778 Server Test Accuracy: 74.05000000000001\n====> i: 95 Loss: 1.236296650767326 Server Test Accuracy: 75.88749999999999\n====> i: 97 Loss: 1.2316453832387924 Server Test Accuracy: 77.63750000000002\n====> i: 99 Loss: 1.2300970947742462 Server Test Accuracy: 75.0125\n====> i: 101 Loss: 1.2275560498237612 Server Test Accuracy: 75.92500000000001\n====> i: 103 Loss: 1.2227810364961624 Server Test Accuracy: 78.87499999999999\n====> i: 105 Loss: 1.223623539805412 Server Test Accuracy: 78.3125\n====> i: 107 Loss: 1.2252915447950363 Server Test Accuracy: 77.85\n====> i: 109 Loss: 1.2229846590757367 Server Test Accuracy: 78.66250000000002\n====> i: 111 Loss: 1.2205044543743135 Server Test Accuracy: 79.27499999999999\n====> i: 113 Loss: 1.2194027072191238 Server Test Accuracy: 79.71250000000002\n====> i: 115 Loss: 1.2191148966550824 Server Test Accuracy: 78.10000000000001\n====> i: 117 Loss: 1.2182491439580916 Server Test Accuracy: 79.05\n====> i: 119 Loss: 1.2185113024711607 Server Test Accuracy: 79.64999999999999\n====> i: 121 Loss: 1.217422643303871 Server Test Accuracy: 80.22500000000001\n====> i: 123 Loss: 1.216540771126747 Server Test Accuracy: 80.4\n====> i: 125 Loss: 1.2151403999328614 Server Test Accuracy: 80.625\n====> i: 127 Loss: 1.2147260928153991 Server Test Accuracy: 79.72500000000001\n====> i: 129 Loss: 1.2147976684570312 Server Test Accuracy: 79.825\n====> i: 131 Loss: 1.2142459934949876 Server Test Accuracy: 80.0\n====> i: 133 Loss: 1.2137442642450333 Server Test Accuracy: 80.275\n====> i: 135 Loss: 1.213301724791527 Server Test Accuracy: 80.55000000000001\n====> i: 137 Loss: 1.212498455643654 Server Test Accuracy: 80.75000000000001\n====> i: 139 Loss: 1.2122552704811096 Server Test Accuracy: 80.52499999999999\n====> i: 141 Loss: 1.2119310289621354 Server Test Accuracy: 80.775\n====> i: 143 Loss: 1.2116680788993837 Server Test Accuracy: 80.77500000000002\n====> i: 145 Loss: 1.2114950096607209 Server Test Accuracy: 80.67500000000001\n====> i: 147 Loss: 1.211105986237526 Server Test Accuracy: 80.78750000000001\n====> i: 149 Loss: 1.2107385671138764 Server Test Accuracy: 80.825\nEXPERIMENT IS FINISHED\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Clustered 2","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\nprint(\"dataset - sampling - sim_type - seed - n_SGD - lr - decay - p - force - mu\")\n# HYPER PARAMETERS\ndataset = \"MNIST_shard\"\nsampling = \"clustered_2\"\nsim_type = \"cosine\"\nseed = int(0)\nn_SGD = int(50)\nlr = float(0.01)\ndecay = float(0.95)\np = float(0.1)\nforce = False\n\ntry:\n    mu = float(sys.argv[10])\nexcept:\n    mu = 0.0\n\"\"\"GET THE HYPERPARAMETERS\"\"\"\nn_iter, batch_size, meas_perf_period = get_hyperparams(dataset, n_SGD)\nn_iter = 150\nprint(\"number of iterations\", n_iter)\nprint(\"batch size\", batch_size)\nprint(\"percentage of sampled clients\", p)\nprint(\"metric_period\", meas_perf_period)\nprint(\"regularization term\", mu)\n\n\n\"\"\"NAME UNDER WHICH THE EXPERIMENT'S VARIABLES WILL BE SAVED\"\"\"\nfile_name = get_file_name(dataset, sampling, sim_type, seed, n_SGD, lr, decay, p, mu)\nprint(file_name)\n\n\n\"\"\"GET THE DATASETS USED FOR THE FL TRAINING\"\"\"\nlist_dls_train, list_dls_test = get_dataloaders(dataset, batch_size)\n\n\n\"\"\"NUMBER OF SAMPLED CLIENTS\"\"\"\nn_sampled = int(p * len(list_dls_train))\nprint(\"number fo sampled clients\", n_sampled)\n\n\n\"\"\"LOAD THE INTIAL GLOBAL MODEL\"\"\"\nmodel_0 = load_model(dataset, seed)\nprint(model_0)\n\n\n\"\"\"1. FEDAVG with random sampling\"\"\"\nif sampling == \"random\" and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_sampling_random(\n        model_0, n_sampled, list_dls_train, list_dls_test, n_iter,\n        n_SGD, lr, file_name, decay, meas_perf_period, mu,\n    )\n\n\n\"\"\"2. FEDAVG with clustered sampling\"\"\"\nif (sampling == \"clustered_1\" or sampling == \"clustered_2\") and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_clustered_sampling(\n        sampling, model_0, n_sampled, list_dls_train, list_dls_test, \n        n_iter, n_SGD, lr, file_name, sim_type, 0, decay, meas_perf_period, mu,\n    )\n\n\n\"\"\"3. FEDAVG with perfect sampling for MNIST-shard\"\"\"\nif (\n    sampling == \"perfect\"\n    and dataset == \"MNIST_shard\"\n    and (not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force)\n):\n    FedProx_sampling_target(\n        model_0, n_sampled, list_dls_train,\n        list_dls_test, n_iter, n_SGD, lr, file_name, decay, mu,\n    )\n\n\n\"\"\"4. FEDAVG with its original sampling scheme sampling clients uniformly\"\"\"\nif sampling == \"FedAvg\" and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_FedAvg_sampling( \n        model_0, n_sampled, list_dls_train, list_dls_test, n_iter,\n        n_SGD, lr, file_name, decay, meas_perf_period, mu,\n    )\n\nprint(\"EXPERIMENT IS FINISHED\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustered 3","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n\nprint(\"dataset - sampling - sim_type - seed - n_SGD - lr - decay - p - force - mu\")\n# HYPER PARAMETERS\ndataset = \"MNIST_shard\"\nsampling = \"random\"\nsim_type = \"cosine\"\nseed = int(0)\nn_SGD = int(50)\nlr = float(0.01)\ndecay = float(0.95)\np = float(0.1)\nforce = False\n\ntry:\n    mu = float(sys.argv[10])\nexcept:\n    mu = 0.0\n\"\"\"GET THE HYPERPARAMETERS\"\"\"\nn_iter, batch_size, meas_perf_period = get_hyperparams(dataset, n_SGD)\nn_iter = 150\nprint(\"number of iterations\", n_iter)\nprint(\"batch size\", batch_size)\nprint(\"percentage of sampled clients\", p)\nprint(\"metric_period\", meas_perf_period)\nprint(\"regularization term\", mu)\n\n\n\"\"\"NAME UNDER WHICH THE EXPERIMENT'S VARIABLES WILL BE SAVED\"\"\"\nfile_name = get_file_name(dataset, sampling, sim_type, seed, n_SGD, lr, decay, p, mu)\nprint(file_name)\n\n\n\"\"\"GET THE DATASETS USED FOR THE FL TRAINING\"\"\"\nlist_dls_train, list_dls_test = get_dataloaders(dataset, batch_size)\n\n\n\"\"\"NUMBER OF SAMPLED CLIENTS\"\"\"\nn_sampled = int(p * len(list_dls_train))\nprint(\"number fo sampled clients\", n_sampled)\n\n\n\"\"\"LOAD THE INTIAL GLOBAL MODEL\"\"\"\nmodel_0 = load_model(dataset, seed)\nprint(model_0)\n\n\n\"\"\"1. FEDAVG with random sampling\"\"\"\nif sampling == \"random\" and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_sampling_random(\n        model_0, n_sampled, list_dls_train, list_dls_test, n_iter,\n        n_SGD, lr, file_name, decay, meas_perf_period, mu,\n    )\n\n\n\"\"\"2. FEDAVG with clustered sampling\"\"\"\nif (sampling == \"clustered_1\" or sampling == \"clustered_2\") and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_clustered_sampling(\n        sampling, model_0, n_sampled, list_dls_train, list_dls_test, \n        n_iter, n_SGD, lr, file_name, sim_type, 0, decay, meas_perf_period, mu,\n    )\n\n\n\"\"\"3. FEDAVG with perfect sampling for MNIST-shard\"\"\"\nif (\n    sampling == \"perfect\"\n    and dataset == \"MNIST_shard\"\n    and (not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force)\n):\n    FedProx_sampling_target(\n        model_0, n_sampled, list_dls_train,\n        list_dls_test, n_iter, n_SGD, lr, file_name, decay, mu,\n    )\n\n\n\"\"\"4. FEDAVG with its original sampling scheme sampling clients uniformly\"\"\"\nif sampling == \"FedAvg\" and (\n    not os.path.exists(f\"saved_exp_info/acc/{file_name}.pkl\") or force\n):\n    FedProx_FedAvg_sampling( \n        model_0, n_sampled, list_dls_train, list_dls_test, n_iter,\n        n_SGD, lr, file_name, decay, meas_perf_period, mu,\n    )\n\nprint(\"EXPERIMENT IS FINISHED\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results Visualization","metadata":{}},{"cell_type":"markdown","source":"### Clustered 1","metadata":{}},{"cell_type":"code","source":"# Clustered 1\ncl1_loss = None\ncl1_acc = None\nwith open('/kaggle/working/saved_exp_info/loss/MNIST_shard_clustered_1_cosine_i100_N50_lr0.01_B50_d0.95_p0.1_m2_0.pkl','rb') as f:\n    cl1_loss = pickle.load(f)\nwith open('/kaggle/working/saved_exp_info/acc/MNIST_shard_clustered_1_cosine_i100_N50_lr0.01_B50_d0.95_p0.1_m2_0.pkl','rb') as f:\n    cl1_acc = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:34:28.629651Z","iopub.execute_input":"2023-11-29T07:34:28.630131Z","iopub.status.idle":"2023-11-29T07:34:28.637111Z","shell.execute_reply.started":"2023-11-29T07:34:28.630096Z","shell.execute_reply":"2023-11-29T07:34:28.635936Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:34:29.869221Z","iopub.execute_input":"2023-11-29T07:34:29.869671Z","iopub.status.idle":"2023-11-29T07:34:29.875312Z","shell.execute_reply.started":"2023-11-29T07:34:29.869639Z","shell.execute_reply":"2023-11-29T07:34:29.874015Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"cl1_loss_res = [np.mean(cl1_loss[i]) for i in range(1,len(cl1_loss),2)]\ncl1_acc_res = [np.mean(cl1_acc[i]) for i in range(1,len(cl1_acc),2)]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:34:30.872488Z","iopub.execute_input":"2023-11-29T07:34:30.873043Z","iopub.status.idle":"2023-11-29T07:34:30.883126Z","shell.execute_reply.started":"2023-11-29T07:34:30.872969Z","shell.execute_reply":"2023-11-29T07:34:30.881509Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Loss\")\nplt.plot(cl1_loss_res)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:35:10.193341Z","iopub.execute_input":"2023-11-29T07:35:10.194332Z","iopub.status.idle":"2023-11-29T07:35:10.825038Z","shell.execute_reply.started":"2023-11-29T07:35:10.194290Z","shell.execute_reply":"2023-11-29T07:35:10.824034Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x79e9823ba380>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"plt.title(\"Accuracy\")\nplt.ylim((0,100))\nplt.plot(cl1_acc_res)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:35:31.098421Z","iopub.execute_input":"2023-11-29T07:35:31.100091Z","iopub.status.idle":"2023-11-29T07:35:31.437082Z","shell.execute_reply.started":"2023-11-29T07:35:31.099943Z","shell.execute_reply":"2023-11-29T07:35:31.435806Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x79e9822dfb50>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}